{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1481543399241_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-mysia.4q0vnhdvcavehkveh1r0ceg34e.cx.internal.cloudapp.net:8088/proxy/application_1481543399241_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.11:30060/node/containerlogs/container_1481543399241_0005_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext available as 'sc'.\n",
      "HiveContext available as 'sqlContext'.\n",
      "u'The Project Gutenberg EBook of Ulysses, by James Joyce'"
     ]
    }
   ],
   "source": [
    "txt = sc.textFile('wasb:///example/data/gutenberg/ulysses.txt')\n",
    "txt.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33055"
     ]
    }
   ],
   "source": [
    "txt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f38b3c88dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data = sc.textFile(\"file:///home/ubuntu/gtest/dist-keras/examples/data/atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data  = csv_data.map(lambda p: p.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = csv_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data = csv_data.filter(lambda p:p != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_csv = csv_data.map(lambda p: Row(EmployeeID = int(p[0]), FirstName = p[1], Title=p[2], State=p[3], Laptop=p[4])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+-------+-------+\n",
      "|EmployeeID|FirstName|Laptop|  State|  Title|\n",
      "+----------+---------+------+-------+-------+\n",
      "|    100000|   138.47| 27.98| 97.827| 51.655|\n",
      "|    100001|  160.937|48.146|103.235| 68.768|\n",
      "|    100002|   -999.0|35.635|125.953|162.172|\n",
      "|    100003|  143.905| 0.414| 80.943| 81.417|\n",
      "|    100004|  175.864|16.405|134.805| 16.915|\n",
      "+----------+---------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeID: long (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- Laptop: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_csv.write.format('orc').options(path=\"hdfs://master:9000/user/hive/warehouse1\").saveAsTable('df3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f38a15c81d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.executor.instances', u'7'),\n",
       " (u'spark.driver.appUIAddress', u'http://172.31.7.225:4040'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'master'),\n",
       " (u'spark.ui.proxyBase', u'/proxy/application_1497490295262_0008'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.driver.port', u'40169'),\n",
       " (u'spark.executor.cores', u'2'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'http://master:8088/proxy/application_1497490295262_0008'),\n",
       " (u'spark.app.name', u'PySparkShell'),\n",
       " (u'spark.app.id', u'application_1497490295262_0008'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.sql.warehouse.dir', u'hdfs://master:9000/user/hive/warehouse'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.host', u'172.31.7.225'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'/home/ubuntu/Download/spark-2.1.1/python/lib/py4j-0.10.4-src.zip:/home/ubuntu/Download/spark-2.1.1/python/:/home/ubuntu/Download/spark-2.1.1/python:/home/ubuntu/anaconda2:/home/ubuntu/anaconda2/lib/python2.7/site-packages<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.4-src.zip'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " (u'hive.metastore.warehouse.dir', u'hdfs://master:9000/user/hive/warehouse')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f38b39e32d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(u'hive.metastore.warehouse.dir',u'hdfs://user/hive/warehouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py',\n",
       " '-f',\n",
       " '/run/user/1000/jupyter/kernel-d74ca269-e32f-435f-9be3-f90a49fb63bb.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Download/hadoop-2.8.0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('HADOOP_HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f38b3c88dd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+-------+\n",
      "|EmployeeID|FirstName| Laptop|  State|  Title|\n",
      "+----------+---------+-------+-------+-------+\n",
      "|    100000|   138.47|  27.98| 97.827| 51.655|\n",
      "|    100001|  160.937| 48.146|103.235| 68.768|\n",
      "|    100002|   -999.0| 35.635|125.953|162.172|\n",
      "|    100003|  143.905|  0.414| 80.943| 81.417|\n",
      "|    100004|  175.864| 16.405|134.805| 16.915|\n",
      "|    100005|   89.744|116.344| 59.149|  13.55|\n",
      "|    100006|  148.754| 106.13|107.782| 28.862|\n",
      "|    100007|  154.916| 29.169| 94.714| 10.418|\n",
      "|    100008|  105.594|  4.288|100.989| 50.559|\n",
      "|    100009|  128.053|193.392| 69.272| 88.941|\n",
      "|    100010|   -999.0| 27.201| 79.692|  86.24|\n",
      "|    100011|  114.744| 30.816| 75.712| 10.286|\n",
      "|    100012|  145.297|106.999|103.565| 64.234|\n",
      "|    100013|   82.488|  8.232| 64.128| 31.663|\n",
      "|    100014|   -999.0| 17.323| 14.398|109.412|\n",
      "|    100015|  111.026| 23.067| 75.271| 32.096|\n",
      "|    100016|  114.256| 47.221| 67.963|  4.351|\n",
      "|    100017|  127.861| 26.967| 77.267| 50.953|\n",
      "|    100018|   -999.0|  5.042| 68.827| 85.186|\n",
      "|    100019|   -999.0| 15.337|115.058| 88.767|\n",
      "+----------+---------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM df3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+-------+\n",
      "|EmployeeID|FirstName| Laptop|  State|  Title|\n",
      "+----------+---------+-------+-------+-------+\n",
      "|    100001|  160.937| 48.146|103.235| 68.768|\n",
      "|    100002|   -999.0| 35.635|125.953|162.172|\n",
      "|    100004|  175.864| 16.405|134.805| 16.915|\n",
      "|    100006|  148.754| 106.13|107.782| 28.862|\n",
      "|    100008|  105.594|  4.288|100.989| 50.559|\n",
      "|    100012|  145.297|106.999|103.565| 64.234|\n",
      "|    100019|   -999.0| 15.337|115.058| 88.767|\n",
      "|    100023|  141.481|174.075|111.581|  0.736|\n",
      "|    100034|  156.894| 71.682|122.894| 39.256|\n",
      "|    100036|  134.153|  9.377| 100.96| 23.856|\n",
      "|    100037|  155.487|  2.768|125.013| 61.072|\n",
      "|    100046|  201.473|  60.49|104.041| 23.002|\n",
      "|    100048|  167.534|  9.909|123.308| 41.277|\n",
      "|    100050|  219.057|  5.506|124.835| 72.461|\n",
      "|    100052|   -999.0| 40.286|136.152| 83.924|\n",
      "|    100054|  219.292|  2.423|177.143|   74.6|\n",
      "|    100076|  239.551|  6.549|193.111|118.683|\n",
      "|    100078|   132.35| 124.84| 106.57| 65.858|\n",
      "|    100086|    199.9| 77.236|167.085| 78.686|\n",
      "|    100089|  235.976| 39.662|175.744| 55.918|\n",
      "+----------+---------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM df3 WHERE State>100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|      df3|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            function|\n",
      "+--------------------+\n",
      "|                   !|\n",
      "|                   %|\n",
      "|                   &|\n",
      "|                   *|\n",
      "|                   +|\n",
      "|                   -|\n",
      "|                   /|\n",
      "|                   <|\n",
      "|                  <=|\n",
      "|                 <=>|\n",
      "|                   =|\n",
      "|                  ==|\n",
      "|                   >|\n",
      "|                  >=|\n",
      "|                   ^|\n",
      "|                 abs|\n",
      "|                acos|\n",
      "|          add_months|\n",
      "|                 and|\n",
      "|approx_count_dist...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW FUNCTIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"CREATE TABLE df2 (foo INT, bar STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|      df2|      false|\n",
      "| default|      df3|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|      df2|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW TABLES '*2'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"CREATE EXTERNAL TABLE page_view (viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING,\\\n",
    "              ip STRING COMMENT 'country of origination') COMMENT 'This is the staging page view table' ROW FORMAT DELIMITED \\\n",
    "              FIELDS TERMINATED BY '\\054' STORED AS TEXTFILE LOCATION 'hdfs://master:9000/user/hive/countries'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|    col_name|\n",
      "+------------+\n",
      "|    viewTime|\n",
      "|      userid|\n",
      "|    page_url|\n",
      "|referrer_url|\n",
      "|          ip|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW COLUMNS FROM page_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+\n",
      "|    col_name|data_type|             comment|\n",
      "+------------+---------+--------------------+\n",
      "|    viewTime|      int|                null|\n",
      "|      userid|   bigint|                null|\n",
      "|    page_url|   string|                null|\n",
      "|referrer_url|   string|                null|\n",
      "|          ip|   string|country of origin...|\n",
      "+------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"DESCRIBE page_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|      df2|      false|\n",
      "| default|      df3|      false|\n",
      "| default|page_view|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW DATABASES\").show()\n",
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"ALTER DATABASE default SET DBPROPERTIES (key1=12)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------------+\n",
      "|database_description_item|database_description_value|\n",
      "+-------------------------+--------------------------+\n",
      "|            Database Name|                   default|\n",
      "|              Description|      Default Hive data...|\n",
      "|                 Location|      hdfs://master:900...|\n",
      "+-------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"DESCRIBE DATABASE default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sqlContext.sql(\"DESCRIBE DATABASE default\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.sql(\"DESCRIBE DATABASE default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_jcols',\n",
       " '_jdf',\n",
       " '_jmap',\n",
       " '_jseq',\n",
       " '_lazy_rdd',\n",
       " '_sc',\n",
       " '_schema',\n",
       " '_sort_cols',\n",
       " 'agg',\n",
       " 'alias',\n",
       " 'approxQuantile',\n",
       " 'cache',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'collect',\n",
       " 'columns',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'createGlobalTempView',\n",
       " 'createOrReplaceTempView',\n",
       " 'createTempView',\n",
       " 'crossJoin',\n",
       " 'crosstab',\n",
       " 'cube',\n",
       " 'describe',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'dropDuplicates',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'explain',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'freqItems',\n",
       " 'groupBy',\n",
       " 'groupby',\n",
       " 'head',\n",
       " 'intersect',\n",
       " 'isLocal',\n",
       " 'isStreaming',\n",
       " 'is_cached',\n",
       " 'join',\n",
       " 'limit',\n",
       " 'na',\n",
       " 'orderBy',\n",
       " 'persist',\n",
       " 'printSchema',\n",
       " 'randomSplit',\n",
       " 'rdd',\n",
       " 'registerTempTable',\n",
       " 'repartition',\n",
       " 'replace',\n",
       " 'rollup',\n",
       " 'sample',\n",
       " 'sampleBy',\n",
       " 'schema',\n",
       " 'select',\n",
       " 'selectExpr',\n",
       " 'show',\n",
       " 'sort',\n",
       " 'sortWithinPartitions',\n",
       " 'sql_ctx',\n",
       " 'stat',\n",
       " 'storageLevel',\n",
       " 'subtract',\n",
       " 'take',\n",
       " 'toDF',\n",
       " 'toJSON',\n",
       " 'toLocalIterator',\n",
       " 'toPandas',\n",
       " 'union',\n",
       " 'unionAll',\n",
       " 'unpersist',\n",
       " 'where',\n",
       " 'withColumn',\n",
       " 'withColumnRenamed',\n",
       " 'withWatermark',\n",
       " 'write',\n",
       " 'writeStream']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(database_description_item=u'Database Name', database_description_value=u'default'),\n",
       " Row(database_description_item=u'Description', database_description_value=u'Default Hive database'),\n",
       " Row(database_description_item=u'Location', database_description_value=u'file:/home/ubuntu/gtest/pySpark-Hive-sql/spark-warehouse')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|       df|      false|\n",
      "| default|      df1|      false|\n",
      "| default|      df2|      false|\n",
      "| default|page_view|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|       df|      false|\n",
      "| default|      df1|      false|\n",
      "| default|      df2|      false|\n",
      "| default|page_view|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 2, 3]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = `a`\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"DROP TABLE page_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"CREATE EXTERNAL TABLE page_view (viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING,\\\n",
    "              ip STRING COMMENT 'country of origination') COMMENT 'This is the staging page view table' ROW FORMAT DELIMITED \\\n",
    "              FIELDS TERMINATED BY '\\054' STORED AS TEXTFILE LOCATION 'hdfs://master:9000/user/hive/countries'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"CREATE EXTERNAL TABLE images (id STRING COMMENT 'id of images', mat STRING COMMENT 'string of list') COMMENT \\\n",
    "'This is used to store images' ROW FORMAT DELIMITED \\\n",
    "FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE LOCATION 'hdfs://master:9000/user/hive/images'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|      df2|      false|\n",
      "| default|      df3|      false|\n",
      "| default|   images|      false|\n",
      "| default|page_view|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 2, 3]'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sqlContext.createDataFrame([('1', '[1, 2, 3]'), ('2', '[4, 5, 6]'), ('3', '[7, 8, 9]')], ['id', 'mat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      mat|\n",
      "+---+---------+\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|[4, 5, 6]|\n",
      "|  3|[7, 8, 9]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, mat: string]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toDF('id', 'mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      mat|\n",
      "+---+---------+\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|[4, 5, 6]|\n",
      "|  3|[7, 8, 9]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.write.mode(\"append\").insertInto('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      mat|\n",
      "+---+---------+\n",
      "|  1|[1, 2, 3]|\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|[4, 5, 6]|\n",
      "|  2|[4, 5, 6]|\n",
      "|  3|[7, 8, 9]|\n",
      "|  3|[7, 8, 9]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM images\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+-------+\n",
      "|EmployeeID|FirstName| Laptop|  State|  Title|\n",
      "+----------+---------+-------+-------+-------+\n",
      "|    100000|   138.47|  27.98| 97.827| 51.655|\n",
      "|    100001|  160.937| 48.146|103.235| 68.768|\n",
      "|    100002|   -999.0| 35.635|125.953|162.172|\n",
      "|    100003|  143.905|  0.414| 80.943| 81.417|\n",
      "|    100004|  175.864| 16.405|134.805| 16.915|\n",
      "|    100005|   89.744|116.344| 59.149|  13.55|\n",
      "|    100006|  148.754| 106.13|107.782| 28.862|\n",
      "|    100007|  154.916| 29.169| 94.714| 10.418|\n",
      "|    100008|  105.594|  4.288|100.989| 50.559|\n",
      "|    100009|  128.053|193.392| 69.272| 88.941|\n",
      "|    100010|   -999.0| 27.201| 79.692|  86.24|\n",
      "|    100011|  114.744| 30.816| 75.712| 10.286|\n",
      "|    100012|  145.297|106.999|103.565| 64.234|\n",
      "|    100013|   82.488|  8.232| 64.128| 31.663|\n",
      "|    100014|   -999.0| 17.323| 14.398|109.412|\n",
      "|    100015|  111.026| 23.067| 75.271| 32.096|\n",
      "|    100016|  114.256| 47.221| 67.963|  4.351|\n",
      "|    100017|  127.861| 26.967| 77.267| 50.953|\n",
      "|    100018|   -999.0|  5.042| 68.827| 85.186|\n",
      "|    100019|   -999.0| 15.337|115.058| 88.767|\n",
      "+----------+---------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM df3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|EmployeeID|   bigint|   null|\n",
      "| FirstName|   string|   null|\n",
      "|    Laptop|   string|   null|\n",
      "|     State|   string|   null|\n",
      "|     Title|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"DESCRIBE TABLE df3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"CREATE TABLE my_table (EmployeeID BIGINT, FirstName STRING, Laptop STRING, State STRING, Title STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|      df2|      false|\n",
      "| default|      df3|      false|\n",
      "| default|   images|      false|\n",
      "| default| my_table|      false|\n",
      "| default|page_view|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|EmployeeID|   bigint|   null|\n",
      "| FirstName|   string|   null|\n",
      "|    Laptop|   string|   null|\n",
      "|     State|   string|   null|\n",
      "|     Title|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"DESCRIBE TABLE my_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sqlContext.sql(\"SELECT * FROM df3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.write.mode(\"append\").insertInto(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"INSERT INTO TABLE images VALUES ('image1', '[1, 2, 3, 4, 5, 6, 7]')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  id|                 mat|\n",
      "+--------------------+--------------------+\n",
      "|[1,2,3],[[1, 2, 3...|                null|\n",
      "|[1,2,3],[[1, 2, 3...|                null|\n",
      "|              image1|[1, 2, 3, 4, 5, 6...|\n",
      "|         1,[1, 2, 3]|                null|\n",
      "|                   1|                null|\n",
      "|           [1, 2, 3]|                null|\n",
      "|                   1|                null|\n",
      "|           [1, 2, 3]|                null|\n",
      "|                   1|                null|\n",
      "|                    |                null|\n",
      "|           [1, 2, 3]|                null|\n",
      "|                   1|                null|\n",
      "|           [1, 2, 3]|                null|\n",
      "|                   1|           [1, 2, 3]|\n",
      "|             [1,2,3]|[[1, 2, 3],[4, 5,...|\n",
      "|             [1,2,3]|[[1, 2, 3],[4, 5,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT * FROM images').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
